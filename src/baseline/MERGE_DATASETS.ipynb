{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['path', 'sentence'],\n",
      "    num_rows: 10906\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\Usuario\\anaconda3\\Lib\\site-packages\\datasets\\table.py:1381: FutureWarning: promote has been superseded by promote_options='default'.\n",
      "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
      "d:\\Users\\Usuario\\anaconda3\\Lib\\site-packages\\datasets\\table.py:1407: FutureWarning: promote has been superseded by promote_options='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict, Audio, concatenate_datasets\n",
    "import os\n",
    "\n",
    "# List of dataset directories\n",
    "data_dirs = [\n",
    "    r\"D:\\Usuario\\Desktop\\Uni\\4th year\\Advanced Machine Learning\\PROJECT GROUP\\ca\",\n",
    "    r\"D:\\Usuario\\Desktop\\Uni\\4th year\\Advanced Machine Learning\\PROJECT GROUP\\ca2\",\n",
    "    r\"D:\\Usuario\\Desktop\\Uni\\4th year\\Advanced Machine Learning\\PROJECT GROUP\\ca3\",\n",
    "]\n",
    "\n",
    "# Function to load a single dataset\n",
    "def load_local_dataset(data_dir):\n",
    "    path = os.path.join(data_dir, \"validated.tsv\")\n",
    "    validated_data = pd.read_csv(path, sep='\\t')\n",
    "\n",
    "    # Keep only necessary columns, such as \"path\" and \"sentence\"\n",
    "    validated_data = validated_data[[\"path\", \"sentence\"]]\n",
    "    validated_data[\"path\"] = validated_data[\"path\"].apply(lambda x: os.path.join(data_dir, \"clips\", x))\n",
    "\n",
    "    # Convert to Hugging Face Dataset\n",
    "    dataset = Dataset.from_pandas(validated_data)\n",
    "\n",
    "    # Ensure audio files are loaded properly\n",
    "    dataset = dataset.cast_column(\"path\", Audio(sampling_rate=16_000))\n",
    "    return dataset\n",
    "\n",
    "# Load all datasets and concatenate them\n",
    "datasets = [load_local_dataset(data_dir) for data_dir in data_dirs]\n",
    "dataset = concatenate_datasets(datasets)\n",
    "\n",
    "# Print the result\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary saved to vocab3.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "# List of dataset directories\n",
    "data_dirs = [\n",
    "    r\"D:\\Usuario\\Desktop\\Uni\\4th year\\Advanced Machine Learning\\PROJECT GROUP\\ca\",\n",
    "    r\"D:\\Usuario\\Desktop\\Uni\\4th year\\Advanced Machine Learning\\PROJECT GROUP\\ca2\",\n",
    "    r\"D:\\Usuario\\Desktop\\Uni\\4th year\\Advanced Machine Learning\\PROJECT GROUP\\ca3\",\n",
    "    # Add more directories as needed\n",
    "]\n",
    "\n",
    "# Initialize an empty set for the vocabulary\n",
    "vocab = set()\n",
    "\n",
    "# Iterate through each dataset directory\n",
    "for data_dir in data_dirs:\n",
    "    # Load the validated.tsv file\n",
    "    validated_path = os.path.join(data_dir, \"validated.tsv\")\n",
    "    if os.path.exists(validated_path):\n",
    "        validated_data = pd.read_csv(validated_path, sep='\\t')\n",
    "\n",
    "        # Extract unique characters from sentences\n",
    "        for sentence in validated_data[\"sentence\"]:\n",
    "            vocab.update(list(sentence))  # Add all characters in the sentence to the vocab set\n",
    "\n",
    "# Convert the set to a sorted list for consistency\n",
    "vocab = sorted(vocab)\n",
    "\n",
    "# Add special tokens\n",
    "vocab_dict = {char: idx for idx, char in enumerate(vocab)}\n",
    "vocab_dict[\"[UNK]\"] = len(vocab_dict)       # Unknown token\n",
    "vocab_dict[\"[PAD]\"] = len(vocab_dict)       # Padding token\n",
    "vocab_dict[\"|\"] = len(vocab_dict)           # Word delimiter (used as a separator for CTC decoding)\n",
    "\n",
    "# Save vocabulary to a JSON file\n",
    "output_path = \"vocab3.json\"\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as vocab_file:\n",
    "    json.dump(vocab_dict, vocab_file)\n",
    "\n",
    "print(f\"Vocabulary saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
